#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AGI Trinity - Continual Learning AGI with LFM2-VL
ì§€ì†í•™ìŠµ ê¸°ë°˜ AGI ì‹œìŠ¤í…œ

LFM2-VL-1.6B ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ì˜ ì§€ì†í•™ìŠµ AGI
"""
import os
import sys
import asyncio
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.prompt import Prompt, Confirm

app = typer.Typer(
    help="ğŸ§  AGI Trinity - Continual Learning AGI with LFM2-VL",
    rich_markup_mode="rich"
)
console = Console()

# Global paths
HOME = Path(os.path.expanduser("~"))
AGI_HOME = HOME / ".trinity"
CONFIG_PATH = Path(__file__).parent / "config" / "lfm2_config.yaml"

# Ensure directories exist
AGI_HOME.mkdir(parents=True, exist_ok=True)


def load_config() -> Dict[str, Any]:
    """ì„¤ì • ë¡œë“œ"""
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {}


# Global agent instance
_agent = None
_learning_engine = None


async def get_agent():
    """ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    global _agent

    if _agent is None:
        from agents.lfm2_adapter import LFM2VLAdapter, LFM2Config

        config = load_config()
        model_config = config.get("model", {})
        gen_config = config.get("generation", {})
        memory_config = config.get("memory", {})
        cl_config = config.get("continual_learning", {})

        lfm2_config = LFM2Config(
            model_id=model_config.get("id", "LiquidAI/LFM2-VL-1.6B"),
            device=model_config.get("device", "auto"),
            dtype=model_config.get("dtype", "bfloat16"),
            max_new_tokens=gen_config.get("max_new_tokens", 512),
            temperature=gen_config.get("temperature", 0.1),
            min_p=gen_config.get("min_p", 0.15),
            repetition_penalty=gen_config.get("repetition_penalty", 1.05),
            memory_path=memory_config.get("storage_path", "~/.trinity/lfm2_memory"),
            enable_continual_learning=cl_config.get("enabled", True),
            learning_rate=cl_config.get("learning_rate", 1e-5)
        )

        _agent = LFM2VLAdapter(lfm2_config=lfm2_config)

    return _agent


async def get_learning_engine():
    """í•™ìŠµ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _learning_engine

    if _learning_engine is None:
        from core.continual_learning import ContinualLearningEngine

        agent = await get_agent()
        _learning_engine = ContinualLearningEngine(model_adapter=agent)

    return _learning_engine


@app.command()
def ask(
    prompt: str = typer.Argument(..., help="ì§ˆë¬¸ ë˜ëŠ” ì‘ì—… ìš”ì²­"),
    image: Optional[str] = typer.Option(None, "--image", "-i", help="ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” URL"),
    save_history: bool = typer.Option(True, "--save/--no-save", help="ëŒ€í™” ê¸°ë¡ ì €ì¥"),
    show_stats: bool = typer.Option(False, "--stats", help="ë©”ëª¨ë¦¬ í†µê³„ í‘œì‹œ")
):
    """
    ğŸ§  AGIì—ê²Œ ì§ˆë¬¸í•˜ê¸°

    í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ëª¨ë“  ìƒí˜¸ì‘ìš©ì€ ì§€ì†í•™ìŠµì— í™œìš©ë©ë‹ˆë‹¤.
    """
    console.print(Panel.fit(
        "[bold blue]AGI Trinity[/bold blue] - Continual Learning AGI\n"
        "[dim]Powered by LFM2-VL-1.6B[/dim]",
        title="ğŸ§ "
    ))

    async def run():
        agent = await get_agent()
        learning_engine = await get_learning_engine()

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        images = None
        if image:
            console.print(f"[dim]Loading image: {image}[/dim]")
            images = [image]

        # ëª¨ë¸ ë¡œë“œ
        with console.status("[bold green]Loading AGI model...[/bold green]"):
            if not agent._is_loaded:
                await agent.load_model()

        # ì¶”ë¡ 
        with console.status("[bold cyan]Thinking...[/bold cyan]"):
            response = await agent.execute(prompt, images)

        # ê²°ê³¼ í‘œì‹œ
        if response.success:
            console.print("\n[bold green]AGI Response:[/bold green]")
            console.print(Panel(
                Markdown(response.content),
                border_style="green"
            ))

            # ë©”íƒ€ë°ì´í„°
            console.print(f"\n[dim]Latency: {response.latency:.2f}s | "
                         f"Tokens: {response.metadata.get('tokens_generated', 'N/A')}[/dim]")

            # í•™ìŠµ ì—”ì§„ì— ê¸°ë¡
            if save_history:
                exp_id = await learning_engine.record_interaction(
                    prompt=prompt,
                    response=response.content,
                    has_image=images is not None
                )
                console.print(f"[dim]Experience ID: {exp_id}[/dim]")

        else:
            console.print(f"\n[bold red]Error:[/bold red] {response.error}")

        # í†µê³„ í‘œì‹œ
        if show_stats:
            stats = agent.get_memory_stats()
            learning_stats = learning_engine.get_learning_stats()

            table = Table(title="Memory & Learning Stats")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="green")

            table.add_row("Experience Buffer", str(stats["experience_buffer_size"]))
            table.add_row("Learned Concepts", str(stats["learned_concepts"]))
            table.add_row("Interactions", str(stats["interaction_count"]))
            table.add_row("Training Count", str(learning_stats["training_count"]))
            table.add_row("Current Difficulty", f"{learning_stats['current_difficulty']:.2f}")

            console.print("\n")
            console.print(table)

    asyncio.run(run())


@app.command()
def feedback(
    experience_id: str = typer.Argument(..., help="ê²½í—˜ ID"),
    quality: float = typer.Option(..., "--quality", "-q", help="í’ˆì§ˆ ì ìˆ˜ (0.0-1.0)"),
    correction: Optional[str] = typer.Option(None, "--correction", "-c", help="ìˆ˜ì •ëœ ì‘ë‹µ"),
    comment: Optional[str] = typer.Option(None, "--comment", help="í”¼ë“œë°± ì½”ë©˜íŠ¸")
):
    """
    ğŸ“ í”¼ë“œë°± ì œê³µí•˜ê¸°

    ì´ì „ ì‘ë‹µì— ëŒ€í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ AGIì˜ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.
    """
    if not 0.0 <= quality <= 1.0:
        console.print("[red]Error: Quality must be between 0.0 and 1.0[/red]")
        raise typer.Exit(1)

    async def run():
        learning_engine = await get_learning_engine()

        await learning_engine.provide_feedback(
            experience_id=experience_id,
            quality_score=quality,
            user_feedback=comment,
            correction=correction
        )

        console.print(f"[green]âœ“ Feedback recorded for {experience_id}[/green]")
        console.print(f"  Quality: {quality:.1%}")
        if correction:
            console.print(f"  Correction provided: {len(correction)} chars")

    asyncio.run(run())


@app.command()
def train(
    force: bool = typer.Option(False, "--force", "-f", help="ê°•ì œ í›ˆë ¨ íŠ¸ë¦¬ê±°"),
    min_quality: float = typer.Option(0.7, "--min-quality", help="ìµœì†Œ í’ˆì§ˆ ì„ê³„ê°’")
):
    """
    ğŸ“ ì§€ì†í•™ìŠµ í›ˆë ¨ ì‹¤í–‰

    ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ê²½í—˜ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
    """
    console.print(Panel.fit(
        "[bold yellow]Starting Continual Learning Training[/bold yellow]",
        title="ğŸ“ Training"
    ))

    async def run():
        learning_engine = await get_learning_engine()

        stats = learning_engine.get_learning_stats()
        console.print(f"Buffer size: {stats['buffer_size']}")
        console.print(f"High quality ratio: {stats['high_quality_ratio']:.1%}")

        if force or stats['buffer_size'] >= 10:
            with console.status("[bold green]Training...[/bold green]"):
                await learning_engine.trigger_training()

            console.print("[green]âœ“ Training completed[/green]")
        else:
            console.print("[yellow]Not enough data for training. Continue interacting with the AGI.[/yellow]")

    asyncio.run(run())


@app.command()
def status():
    """
    ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    """
    table = Table(title="ğŸ§  AGI Trinity Status")
    table.add_column("Component", style="cyan")
    table.add_column("Status", style="green")
    table.add_column("Details", style="white")

    # Config check
    config_exists = CONFIG_PATH.exists()
    table.add_row(
        "Configuration",
        "âœ… Ready" if config_exists else "âš ï¸ Using defaults",
        str(CONFIG_PATH) if config_exists else "Default config"
    )

    # Memory check
    memory_path = AGI_HOME / "lfm2_memory"
    memory_exists = memory_path.exists()
    if memory_exists:
        exp_files = list(memory_path.glob("experiences_*.json"))
        table.add_row(
            "Memory Storage",
            f"ğŸ“Š {len(exp_files)} experience files",
            str(memory_path)
        )
    else:
        table.add_row(
            "Memory Storage",
            "ğŸ“­ Empty",
            "No experiences yet"
        )

    # Model checkpoint
    checkpoint_path = memory_path / "model_checkpoint"
    if checkpoint_path.exists():
        table.add_row(
            "Model Checkpoint",
            "âœ… Available",
            str(checkpoint_path)
        )
    else:
        table.add_row(
            "Model Checkpoint",
            "ğŸ“¦ Using base model",
            "LiquidAI/LFM2-VL-1.6B"
        )

    # Knowledge
    knowledge_path = AGI_HOME / "learning" / "knowledge" / "knowledge_graph.json"
    if knowledge_path.exists():
        with open(knowledge_path, 'r') as f:
            kg = json.load(f)
        table.add_row(
            "Knowledge Graph",
            f"ğŸ§  {len(kg)} concepts",
            str(knowledge_path)
        )
    else:
        table.add_row(
            "Knowledge Graph",
            "ğŸ“­ Empty",
            "No concepts learned yet"
        )

    # GPU
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            table.add_row(
                "GPU",
                f"âœ… {gpu_name}",
                f"{gpu_mem:.1f} GB"
            )
        else:
            table.add_row("GPU", "âŒ Not available", "CPU mode")
    except ImportError:
        table.add_row("PyTorch", "âŒ Not installed", "pip install torch")

    console.print(table)


@app.command()
def chat():
    """
    ğŸ’¬ ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ

    ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ í†µí•´ AGIì™€ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.
    """
    console.print(Panel.fit(
        "[bold blue]AGI Trinity Chat Mode[/bold blue]\n"
        "[dim]Type 'exit' to quit, 'stats' for statistics[/dim]",
        title="ğŸ’¬ Chat"
    ))

    async def run():
        agent = await get_agent()
        learning_engine = await get_learning_engine()

        # ëª¨ë¸ ë¡œë“œ
        with console.status("[bold green]Loading AGI...[/bold green]"):
            if not agent._is_loaded:
                await agent.load_model()

        console.print("[green]AGI ready! Start chatting.[/green]\n")

        conversation_history = []

        while True:
            try:
                user_input = Prompt.ask("[bold cyan]You[/bold cyan]")

                if user_input.lower() == 'exit':
                    console.print("[yellow]Goodbye![/yellow]")
                    break

                if user_input.lower() == 'stats':
                    stats = learning_engine.get_learning_stats()
                    console.print(f"\n[dim]Buffer: {stats['buffer_size']} | "
                                 f"Concepts: {stats['knowledge_concepts']} | "
                                 f"Difficulty: {stats['current_difficulty']:.2f}[/dim]\n")
                    continue

                if user_input.lower() == 'train':
                    await learning_engine.trigger_training()
                    console.print("[green]Training triggered[/green]\n")
                    continue

                if not user_input.strip():
                    continue

                # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                context = ""
                if conversation_history:
                    context = "\n".join([
                        f"User: {h['user']}\nAssistant: {h['assistant']}"
                        for h in conversation_history[-3:]  # ìµœê·¼ 3ê°œ
                    ])
                    context += "\n\n"

                full_prompt = context + user_input

                # ì‘ë‹µ ìƒì„±
                with console.status("[bold cyan]Thinking...[/bold cyan]"):
                    response = await agent.execute(full_prompt)

                if response.success:
                    console.print(f"\n[bold green]AGI[/bold green]: {response.content}\n")

                    # ê¸°ë¡
                    conversation_history.append({
                        "user": user_input,
                        "assistant": response.content
                    })

                    # í•™ìŠµ ì—”ì§„ì— ê¸°ë¡
                    await learning_engine.record_interaction(
                        prompt=user_input,
                        response=response.content
                    )
                else:
                    console.print(f"\n[red]Error: {response.error}[/red]\n")

            except KeyboardInterrupt:
                console.print("\n[yellow]Interrupted. Type 'exit' to quit.[/yellow]")
                continue

    asyncio.run(run())


@app.command()
def learn(
    topic: str = typer.Argument(..., help="í•™ìŠµí•  ì£¼ì œ"),
    depth: int = typer.Option(3, "--depth", "-d", help="í•™ìŠµ ê¹Šì´ (1-5)")
):
    """
    ğŸ“š íŠ¹ì • ì£¼ì œ í•™ìŠµí•˜ê¸°

    AGIê°€ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ìê¸° ì£¼ë„ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    console.print(Panel.fit(
        f"[bold blue]Learning Topic: {topic}[/bold blue]\n"
        f"[dim]Depth: {depth}[/dim]",
        title="ğŸ“š Self-Learning"
    ))

    async def run():
        agent = await get_agent()
        learning_engine = await get_learning_engine()

        if not agent._is_loaded:
            with console.status("[bold green]Loading AGI...[/bold green]"):
                await agent.load_model()

        # ìê¸° ì£¼ë„ í•™ìŠµ ë£¨í”„
        for level in range(1, depth + 1):
            console.print(f"\n[bold]Level {level}/{depth}[/bold]")

            # í•™ìŠµ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = [
                f"What is {topic}? Explain the fundamentals.",
                f"What are the key concepts and principles of {topic}?",
                f"How does {topic} relate to other fields?",
                f"What are practical applications of {topic}?",
                f"What are advanced topics in {topic}?"
            ]

            prompt = prompts[min(level - 1, len(prompts) - 1)]

            with console.status(f"[cyan]Learning: {prompt[:50]}...[/cyan]"):
                response = await agent.execute(prompt)

            if response.success:
                console.print(Panel(
                    Markdown(response.content[:500] + "..." if len(response.content) > 500 else response.content),
                    title=f"Level {level} Understanding"
                ))

                # í•™ìŠµ ê¸°ë¡
                await learning_engine.record_interaction(
                    prompt=prompt,
                    response=response.content,
                    domain=topic
                )

        console.print("\n[green]âœ“ Self-learning session completed[/green]")

    asyncio.run(run())


@app.command()
def knowledge(
    query: Optional[str] = typer.Argument(None, help="ê²€ìƒ‰í•  ê°œë…"),
    list_all: bool = typer.Option(False, "--list", "-l", help="ëª¨ë“  ê°œë… ë‚˜ì—´")
):
    """
    ğŸ§  ì§€ì‹ ê·¸ë˜í”„ ì¡°íšŒ

    AGIê°€ í•™ìŠµí•œ ê°œë…ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    async def run():
        learning_engine = await get_learning_engine()

        if list_all:
            # ëª¨ë“  ê°œë… ë‚˜ì—´
            concepts = learning_engine.knowledge.knowledge_graph

            if not concepts:
                console.print("[yellow]No concepts learned yet.[/yellow]")
                return

            table = Table(title="ğŸ§  Learned Concepts")
            table.add_column("Concept", style="cyan")
            table.add_column("Domain", style="green")
            table.add_column("Access Count", style="yellow")

            for concept_id, data in concepts.items():
                table.add_row(
                    data.get("name", concept_id)[:30],
                    data.get("domain", "general"),
                    str(data.get("access_count", 0))
                )

            console.print(table)

        elif query:
            # ê°œë… ê²€ìƒ‰
            results = await learning_engine.get_relevant_knowledge(query)

            if not results:
                console.print(f"[yellow]No concepts found for '{query}'[/yellow]")
                return

            console.print(f"\n[bold]Found {len(results)} related concepts:[/bold]\n")

            for i, concept in enumerate(results, 1):
                console.print(Panel(
                    f"[bold]{concept.get('name', 'Unknown')}[/bold]\n\n"
                    f"{concept.get('definition', '')[:300]}",
                    title=f"#{i}"
                ))

        else:
            console.print("[yellow]Please provide a query or use --list[/yellow]")

    asyncio.run(run())


@app.command()
def export(
    output_path: str = typer.Argument("agi_export.json", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ"),
    include_model: bool = typer.Option(False, "--include-model", help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í¬í•¨")
):
    """
    ğŸ“¤ AGI ìƒíƒœ ë‚´ë³´ë‚´ê¸°

    í•™ìŠµëœ ì§€ì‹ê³¼ ê²½í—˜ì„ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
    """
    async def run():
        learning_engine = await get_learning_engine()
        agent = await get_agent()

        export_data = {
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0",
            "learning_stats": learning_engine.get_learning_stats(),
            "memory_stats": agent.get_memory_stats(),
            "knowledge_graph": learning_engine.knowledge.knowledge_graph,
            "long_term_memory": agent.long_term_memory
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        console.print(f"[green]âœ“ Exported to {output_path}[/green]")

    asyncio.run(run())


if __name__ == "__main__":
    app()
