#!/usr/bin/env python3
"""
AGI Trinity - Computer Use Learner (VLA Style)
ë§ˆìš°ìŠ¤/í‚¤ë³´ë“œ ì¡°ì‘ + ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ ì»´í“¨í„° ì‚¬ìš©ë²• í•™ìŠµ
OpenAI VLA / RT-2 ìŠ¤íƒ€ì¼ ë°ì´í„° ìˆ˜ì§‘

ë°ì´í„° í˜•ì‹:
{
    "screenshot_before": image,
    "action": {"type": "click/type/scroll", "x": 100, "y": 200, "text": "..."},
    "screenshot_after": image,
    "task_description": "..."
}
"""
import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
import base64

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("playwright not installed. Run: pip install playwright && playwright install chromium")
    sys.exit(1)

try:
    import pyautogui
    from pynput import mouse, keyboard
    PYNPUT_AVAILABLE = True
except ImportError:
    PYNPUT_AVAILABLE = False
    print("pyautogui/pynput not installed. Run: pip install pyautogui pynput")

sys.path.insert(0, str(Path(__file__).parent))


class ComputerUseLearner:
    """VLA ìŠ¤íƒ€ì¼ ì»´í“¨í„° ì‚¬ìš© í•™ìŠµê¸°"""

    def __init__(self, data_dir: str = "/home/kim/agi/computer_use_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.agi = None
        self.recording = False
        self.episode_data = []
        self.current_episode = 0
        self.last_screenshot = None
        self.action_buffer = []

    async def setup_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ§  Loading VL model...")
        start = time.time()

        from agents.lfm2_adapter import LFM2VLAdapter, LFM2Config

        config = LFM2Config(
            model_id="LiquidAI/LFM2-VL-1.6B",
            enable_continual_learning=True
        )
        self.agi = LFM2VLAdapter(lfm2_config=config)
        await self.agi.load_model()
        print(f"âœ… Model loaded in {time.time() - start:.1f}s")

    def take_screenshot(self) -> str:
        """ìŠ¤í¬ë¦°ìƒ· ì°ê³  base64ë¡œ ë°˜í™˜"""
        screenshot_path = self.data_dir / f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.png"
        pyautogui.screenshot(str(screenshot_path))
        return str(screenshot_path)

    def start_recording(self, task_description: str = ""):
        """ì¡°ì‘ ê¸°ë¡ ì‹œì‘"""
        self.recording = True
        self.current_episode += 1
        self.episode_data = []
        self.task_description = task_description

        print(f"\nğŸ”´ Recording Episode {self.current_episode}")
        if task_description:
            print(f"   Task: {task_description}")

        # ì´ˆê¸° ìŠ¤í¬ë¦°ìƒ·
        self.last_screenshot = self.take_screenshot()

        # ë§ˆìš°ìŠ¤/í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ ì‹œì‘
        self._start_listeners()

    def stop_recording(self):
        """ê¸°ë¡ ì¤‘ì§€"""
        self.recording = False
        self._stop_listeners()

        # ì—í”¼ì†Œë“œ ì €ì¥
        episode_file = self.data_dir / f"episode_{self.current_episode:04d}.json"
        with open(episode_file, 'w', encoding='utf-8') as f:
            json.dump({
                "episode_id": self.current_episode,
                "task_description": self.task_description,
                "actions": self.episode_data,
                "timestamp": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)

        print(f"â¹ï¸ Stopped. Saved {len(self.episode_data)} actions to {episode_file}")

    def _start_listeners(self):
        """ë§ˆìš°ìŠ¤/í‚¤ë³´ë“œ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì‹œì‘"""
        def on_click(x, y, button, pressed):
            if not self.recording or not pressed:
                return
            self._record_action({
                "type": "click",
                "x": x,
                "y": y,
                "button": str(button)
            })

        def on_key(key):
            if not self.recording:
                return
            try:
                char = key.char
            except AttributeError:
                char = str(key)
            self._record_action({
                "type": "keypress",
                "key": char
            })

        self.mouse_listener = mouse.Listener(on_click=on_click)
        self.keyboard_listener = keyboard.Listener(on_press=on_key)

        self.mouse_listener.start()
        self.keyboard_listener.start()

    def _stop_listeners(self):
        """ë¦¬ìŠ¤ë„ˆ ì¤‘ì§€"""
        if hasattr(self, 'mouse_listener'):
            self.mouse_listener.stop()
        if hasattr(self, 'keyboard_listener'):
            self.keyboard_listener.stop()

    def _record_action(self, action: dict):
        """ì•¡ì…˜ ê¸°ë¡"""
        # í˜„ì¬ ìŠ¤í¬ë¦°ìƒ·
        screenshot_after = self.take_screenshot()

        action_data = {
            "screenshot_before": self.last_screenshot,
            "action": action,
            "screenshot_after": screenshot_after,
            "timestamp": datetime.now().isoformat()
        }

        self.episode_data.append(action_data)
        self.last_screenshot = screenshot_after

        action_str = f"{action['type']}"
        if action['type'] == 'click':
            action_str += f" ({action['x']}, {action['y']})"
        elif action['type'] == 'keypress':
            action_str += f" [{action['key']}]"

        print(f"   ğŸ“ {action_str}")

    async def analyze_screenshot(self, screenshot_path: str) -> str:
        """ìŠ¤í¬ë¦°ìƒ· ë¶„ì„"""
        if not self.agi:
            return "ëª¨ë¸ ë¯¸ë¡œë“œ"

        try:
            response = await self.agi.execute(
                "ì´ í™”ë©´ì—ì„œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                images=[screenshot_path]
            )
            return response.content[:200]
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

    async def autonomous_task(self, task: str, max_steps: int = 10):
        """ììœ¨ íƒœìŠ¤í¬ ìˆ˜í–‰ (AIê°€ ì§ì ‘ ì¡°ì‘)"""
        print(f"\nğŸ¤– Autonomous Task: {task}")

        for step in range(max_steps):
            # í˜„ì¬ í™”ë©´ ìº¡ì²˜
            screenshot = self.take_screenshot()

            # AIì—ê²Œ ë‹¤ìŒ í–‰ë™ ì§ˆë¬¸
            prompt = f"""í˜„ì¬ í™”ë©´ì„ ë³´ê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”: {task}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ACTION: click/type/scroll/done
X: (í´ë¦­ xì¢Œí‘œ, 0-1920)
Y: (í´ë¦­ yì¢Œí‘œ, 0-1080)
TEXT: (ì…ë ¥í•  í…ìŠ¤íŠ¸, typeì¸ ê²½ìš°)
REASON: (ì´ìœ )"""

            try:
                response = await self.agi.execute(prompt, images=[screenshot])
                print(f"   Step {step+1}: {response.content[:100]}...")

                # ì‘ë‹µ íŒŒì‹±
                action = self._parse_action(response.content)
                if action['type'] == 'done':
                    print("   âœ… Task completed!")
                    break

                # ì•¡ì…˜ ìˆ˜í–‰
                await self._execute_action(action)
                await asyncio.sleep(1)

            except Exception as e:
                print(f"   âŒ Error: {e}")
                break

    def _parse_action(self, text: str) -> dict:
        """ì‘ë‹µì—ì„œ ì•¡ì…˜ íŒŒì‹±"""
        action = {"type": "done"}

        lines = text.upper().split('\n')
        for line in lines:
            if line.startswith('ACTION:'):
                action['type'] = line.split(':')[1].strip().lower()
            elif line.startswith('X:'):
                try:
                    action['x'] = int(line.split(':')[1].strip())
                except:
                    pass
            elif line.startswith('Y:'):
                try:
                    action['y'] = int(line.split(':')[1].strip())
                except:
                    pass
            elif line.startswith('TEXT:'):
                action['text'] = line.split(':')[1].strip()

        return action

    async def _execute_action(self, action: dict):
        """ì•¡ì…˜ ìˆ˜í–‰"""
        action_type = action.get('type', 'done')

        if action_type == 'click':
            x, y = action.get('x', 500), action.get('y', 500)
            print(f"   ğŸ–±ï¸ Clicking ({x}, {y})")
            pyautogui.click(x, y)

        elif action_type == 'type':
            text = action.get('text', '')
            print(f"   âŒ¨ï¸ Typing: {text[:30]}...")
            pyautogui.write(text, interval=0.05)

        elif action_type == 'scroll':
            print("   ğŸ”„ Scrolling")
            pyautogui.scroll(-3)

    async def demo_mode(self):
        """ë°ëª¨ ëª¨ë“œ - ì‹¤ì‹œê°„ í™”ë©´ ë¶„ì„"""
        print("\nğŸ® Demo Mode - ì‹¤ì‹œê°„ í™”ë©´ ë¶„ì„")
        print("   Ctrl+Cë¡œ ì¢…ë£Œ")

        while True:
            screenshot = self.take_screenshot()
            analysis = await self.analyze_screenshot(screenshot)
            print(f"\nğŸ“¸ í™”ë©´ ë¶„ì„: {analysis}")
            await asyncio.sleep(5)


async def main():
    import argparse
    parser = argparse.ArgumentParser(description="Computer Use Learner - VLA ìŠ¤íƒ€ì¼")
    parser.add_argument("--mode", choices=["record", "demo", "auto"], default="demo")
    parser.add_argument("--task", type=str, default="", help="Task description")
    args = parser.parse_args()

    if not PYNPUT_AVAILABLE:
        print("âŒ pynputì´ í•„ìš”í•©ë‹ˆë‹¤: pip install pynput pyautogui")
        return

    learner = ComputerUseLearner()

    try:
        await learner.setup_model()

        if args.mode == "demo":
            await learner.demo_mode()
        elif args.mode == "record":
            learner.start_recording(args.task)
            print("Press Ctrl+C to stop recording...")
            while True:
                await asyncio.sleep(1)
        elif args.mode == "auto":
            if not args.task:
                args.task = "ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  êµ¬ê¸€ì—ì„œ 'AI news' ê²€ìƒ‰"
            await learner.autonomous_task(args.task)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Stopping...")
        if learner.recording:
            learner.stop_recording()


if __name__ == "__main__":
    asyncio.run(main())
