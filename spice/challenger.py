"""
SPICE Challenger - Task Generator from Corpus

The Challenger role in SPICE self-play:
1. Mines documents from corpus
2. Generates diverse reasoning tasks
3. Creates challenges of calibrated difficulty
4. Provides ground-truth answers for verification

Based on Meta's SPICE paper (arXiv:2510.24684)
"""

import json
import os
import random
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from .corpus import Document


@dataclass
class Task:
    """A reasoning task generated by Challenger"""
    id: str
    task_type: str  # qa, summarization, reasoning, math, code
    question: str
    ground_truth: str
    source_doc_id: str
    difficulty: float  # 0.0 to 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict:
        return {
            "id": self.id,
            "task_type": self.task_type,
            "question": self.question,
            "ground_truth": self.ground_truth,
            "source_doc_id": self.source_doc_id,
            "difficulty": self.difficulty,
            "metadata": self.metadata
        }


class Challenger:
    """
    SPICE Challenger - Generates reasoning tasks from corpus.

    The Challenger mines documents and creates diverse tasks:
    - Q&A from document content
    - Summarization tasks
    - Logical reasoning
    - Math word problems
    - Code understanding

    Key insight from SPICE: Tasks should be grounded in real
    documents to prevent hallucination and ensure verifiability.
    """

    # Task generation templates
    TASK_TEMPLATES = {
        "qa_factual": [
            "Based on the following context, {question}\n\nContext: {context}",
            "Answer this question using only the provided information:\n{question}\n\nInformation: {context}",
        ],
        "qa_inference": [
            "What can be inferred from the following text about {topic}?\n\nText: {context}",
            "Based on the text below, explain {topic}:\n\n{context}",
        ],
        "summarization": [
            "Summarize the key points from the following text in 2-3 sentences:\n\n{context}",
            "Provide a brief summary of:\n\n{context}",
        ],
        "reasoning": [
            "Given the following information, what logical conclusions can be drawn?\n\n{context}",
            "Analyze the following and explain the reasoning:\n\n{context}",
        ],
    }

    def __init__(self, llm_model: str = "local", data_dir: str = "data/spice"):
        self.llm_model = llm_model
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)

        self._task_counter = 0
        self.generated_tasks: List[Task] = []
        self._llm = None

    async def generate_tasks(self, documents: List[Document],
                            count: int = 10,
                            task_types: Optional[List[str]] = None) -> List[Task]:
        """
        Generate reasoning tasks from documents.

        Args:
            documents: Source documents from corpus
            count: Number of tasks to generate
            task_types: Types of tasks to generate (None = all types)

        Returns:
            List of generated Task objects
        """
        if not documents:
            return []

        if task_types is None:
            task_types = ["qa_factual", "qa_inference", "summarization", "reasoning"]

        tasks = []

        for _ in range(count):
            # Select random document
            doc = random.choice(documents)

            # Select task type
            task_type = random.choice(task_types)

            # Generate task
            task = await self._generate_single_task(doc, task_type)
            if task:
                tasks.append(task)
                self.generated_tasks.append(task)

        self._save_tasks()
        return tasks

    async def _generate_single_task(self, doc: Document, task_type: str) -> Optional[Task]:
        """Generate a single task from document"""

        # Extract relevant context from document
        context = self._extract_context(doc.content)

        if len(context) < 50:
            return None

        # Generate question and ground truth
        question, ground_truth = await self._create_qa_pair(context, task_type, doc.title)

        if not question or not ground_truth:
            return None

        self._task_counter += 1

        # Calculate difficulty based on content complexity
        difficulty = self._estimate_difficulty(context, task_type)

        return Task(
            id=f"task_{self._task_counter}",
            task_type=task_type,
            question=question,
            ground_truth=ground_truth,
            source_doc_id=doc.id,
            difficulty=difficulty,
            metadata={
                "doc_title": doc.title,
                "doc_domain": doc.domain,
                "generated_at": datetime.now().isoformat()
            }
        )

    def _extract_context(self, content: str, max_length: int = 1000) -> str:
        """Extract relevant context window from content"""

        # Split into paragraphs
        paragraphs = content.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 50]

        if not paragraphs:
            paragraphs = content.split('\n')
            paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 30]

        if not paragraphs:
            return content[:max_length]

        # Select random paragraph(s) up to max_length
        selected = []
        current_length = 0

        # Shuffle and select
        random.shuffle(paragraphs)

        for p in paragraphs:
            if current_length + len(p) > max_length:
                break
            selected.append(p)
            current_length += len(p)

        return '\n\n'.join(selected) if selected else paragraphs[0][:max_length]

    async def _create_qa_pair(self, context: str, task_type: str,
                             title: str) -> Tuple[Optional[str], Optional[str]]:
        """Create question-answer pair from context"""

        # Try LLM-based generation first
        if self.llm_model != "local":
            return await self._llm_generate_qa(context, task_type)

        # Fallback to template-based generation
        return self._template_generate_qa(context, task_type, title)

    async def _llm_generate_qa(self, context: str,
                               task_type: str) -> Tuple[Optional[str], Optional[str]]:
        """Use LLM to generate Q&A pair"""
        try:
            if self._llm is None:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                import torch

                model_id = "Qwen/Qwen2.5-1.5B-Instruct"
                self._llm = {
                    "tokenizer": AutoTokenizer.from_pretrained(model_id),
                    "model": AutoModelForCausalLM.from_pretrained(
                        model_id,
                        torch_dtype=torch.bfloat16,
                        device_map="auto"
                    )
                }

            prompt = f"""Generate a {task_type} task based on this text:

{context}

Output format:
QUESTION: [your question]
ANSWER: [correct answer based on the text]"""

            inputs = self._llm["tokenizer"](prompt, return_tensors="pt").to(
                self._llm["model"].device
            )

            outputs = self._llm["model"].generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7
            )

            response = self._llm["tokenizer"].decode(outputs[0], skip_special_tokens=True)

            # Parse response
            if "QUESTION:" in response and "ANSWER:" in response:
                parts = response.split("ANSWER:")
                question = parts[0].split("QUESTION:")[-1].strip()
                answer = parts[1].strip()
                return question, answer

        except Exception as e:
            print(f"[Challenger] LLM generation failed: {e}")

        return self._template_generate_qa(context, task_type, "Document")

    def _template_generate_qa(self, context: str, task_type: str,
                              title: str) -> Tuple[Optional[str], Optional[str]]:
        """Template-based Q&A generation (fallback)"""

        # Extract key sentences
        sentences = [s.strip() for s in context.split('.') if len(s.strip()) > 20]

        if not sentences:
            return None, None

        if task_type == "qa_factual":
            # Pick a sentence and create question about it
            sentence = random.choice(sentences[:5])  # Use early sentences

            # Simple question generation
            question = f"According to the text about {title}, what is described in the following passage?\n\n{context[:500]}"
            ground_truth = sentence

        elif task_type == "qa_inference":
            question = f"What can be inferred from the following text?\n\n{context[:500]}"
            # Use main point as answer
            ground_truth = sentences[0] if sentences else context[:200]

        elif task_type == "summarization":
            question = f"Summarize the following text in 2-3 sentences:\n\n{context}"
            # Create simple summary from first few sentences
            ground_truth = '. '.join(sentences[:3]) + '.' if len(sentences) >= 3 else context[:300]

        elif task_type == "reasoning":
            question = f"Based on the following information, explain the main concept:\n\n{context[:500]}"
            ground_truth = sentences[0] if sentences else context[:200]

        else:
            question = f"What is this text about?\n\n{context[:500]}"
            ground_truth = sentences[0] if sentences else context[:200]

        return question, ground_truth

    def _estimate_difficulty(self, context: str, task_type: str) -> float:
        """Estimate task difficulty based on content"""

        difficulty = 0.5  # Base difficulty

        # Longer context = harder
        if len(context) > 500:
            difficulty += 0.1
        if len(context) > 1000:
            difficulty += 0.1

        # Complex vocabulary = harder
        words = context.split()
        avg_word_len = sum(len(w) for w in words) / len(words) if words else 0
        if avg_word_len > 6:
            difficulty += 0.1

        # Task type affects difficulty
        type_difficulty = {
            "qa_factual": 0.0,
            "summarization": 0.1,
            "qa_inference": 0.2,
            "reasoning": 0.3
        }
        difficulty += type_difficulty.get(task_type, 0)

        return min(1.0, max(0.0, difficulty))

    def calibrate_difficulty(self, task: Task, reasoner_score: float):
        """
        Calibrate task difficulty based on Reasoner performance.

        SPICE insight: Use Reasoner's performance to adjust
        difficulty estimates for curriculum learning.
        """
        # If Reasoner got it right easily, increase difficulty estimate
        if reasoner_score > 0.8:
            task.difficulty = max(0.0, task.difficulty - 0.1)
        # If Reasoner struggled, decrease difficulty estimate
        elif reasoner_score < 0.3:
            task.difficulty = min(1.0, task.difficulty + 0.1)

    def get_curriculum_tasks(self, target_difficulty: float,
                            count: int = 5) -> List[Task]:
        """
        Get tasks matching target difficulty for curriculum learning.

        SPICE insight: Adaptive curriculum based on model's
        current capability level.
        """
        # Filter tasks by difficulty
        tolerance = 0.2
        matching = [
            t for t in self.generated_tasks
            if abs(t.difficulty - target_difficulty) < tolerance
        ]

        if len(matching) < count:
            # Include nearby difficulties
            matching = sorted(
                self.generated_tasks,
                key=lambda t: abs(t.difficulty - target_difficulty)
            )

        return matching[:count]

    def _save_tasks(self):
        """Save generated tasks to disk"""
        tasks_file = os.path.join(self.data_dir, "generated_tasks.json")
        data = {
            "tasks": [t.to_dict() for t in self.generated_tasks],
            "task_counter": self._task_counter
        }
        with open(tasks_file, "w") as f:
            json.dump(data, f, indent=2)

    def _load_tasks(self):
        """Load tasks from disk"""
        tasks_file = os.path.join(self.data_dir, "generated_tasks.json")
        if os.path.exists(tasks_file):
            with open(tasks_file, "r") as f:
                data = json.load(f)

            self.generated_tasks = [
                Task(**t) for t in data.get("tasks", [])
            ]
            self._task_counter = data.get("task_counter", 0)

    def __repr__(self) -> str:
        return f"Challenger({len(self.generated_tasks)} tasks)"
