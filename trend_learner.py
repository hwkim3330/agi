#!/usr/bin/env python3
"""
AGI Trinity - Trend Learner
ì‹¤ì‹œê°„ íŠ¸ë Œë“œ/ë‰´ìŠ¤ í”¼ë“œ í•™ìŠµê¸°
- Reddit, Hacker News, Google Trends
- ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´
- í•­ìƒ ìƒˆë¡œìš´ ì½˜í…ì¸  í•™ìŠµ
"""
import asyncio
import json
import random
import sys
import time
from datetime import datetime
from pathlib import Path

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("playwright not installed. Run: pip install playwright && playwright install chromium")
    sys.exit(1)

sys.path.insert(0, str(Path(__file__).parent))


# ì‹¤ì‹œê°„ í”¼ë“œ ì†ŒìŠ¤ë“¤
FEED_SOURCES = {
    "reddit_ml": {
        "url": "https://www.reddit.com/r/MachineLearning/new/",
        "selector": "a[data-click-id='body']",
        "name": "Reddit ML"
    },
    "reddit_tech": {
        "url": "https://www.reddit.com/r/technology/new/",
        "selector": "a[data-click-id='body']",
        "name": "Reddit Tech"
    },
    "hackernews": {
        "url": "https://news.ycombinator.com/newest",
        "selector": "a.titleline > a",
        "name": "Hacker News"
    },
    "arxiv_ai": {
        "url": "https://arxiv.org/list/cs.AI/recent",
        "selector": "a[title='Abstract']",
        "name": "arXiv AI"
    },
    "naver_news": {
        "url": "https://news.naver.com/section/105",  # IT/ê³¼í•™
        "selector": "a.sa_text_title",
        "name": "ë„¤ì´ë²„ ITë‰´ìŠ¤"
    },
    "google_trends": {
        "url": "https://trends.google.com/trending?geo=KR",
        "selector": "a[href*='/trending']",
        "name": "Google Trends KR"
    }
}


class TrendLearner:
    """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í•™ìŠµê¸°"""

    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser = None
        self.page = None
        self.agi = None
        self.learning_history = []
        self.seen_urls = set()  # ì¤‘ë³µ ë°©ì§€

    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 900},
            user_agent="Mozilla/5.0 (X11; Linux x86_64) Chrome/120.0.0.0"
        )
        self.page = await self.context.new_page()
        print("ğŸŒ Browser ready")

    async def setup_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ§  Loading model...")
        start = time.time()

        from agents.lfm2_adapter import LFM2VLAdapter, LFM2Config

        config = LFM2Config(
            model_id="LiquidAI/LFM2-VL-1.6B",
            enable_continual_learning=True
        )
        self.agi = LFM2VLAdapter(lfm2_config=config)
        await self.agi.load_model()

        print(f"âœ… Model loaded in {time.time() - start:.1f}s")

    async def analyze_text(self, text: str, max_length: int = 2000) -> str:
        """í…ìŠ¤íŠ¸ ë¶„ì„"""
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ 100ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”: {text[:max_length]}"
        try:
            response = await self.agi.execute(prompt)
            return response.content[:300]
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

    async def fetch_feed_links(self, source_key: str) -> list:
        """í”¼ë“œì—ì„œ ìƒˆ ë§í¬ ê°€ì ¸ì˜¤ê¸°"""
        source = FEED_SOURCES.get(source_key)
        if not source:
            return []

        print(f"\nğŸ“¡ Fetching: {source['name']}")

        try:
            await self.page.goto(source['url'], wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(2)

            # ë§í¬ ì¶”ì¶œ
            links = await self.page.evaluate(f"""
                () => {{
                    const elements = document.querySelectorAll("{source['selector']}");
                    return Array.from(elements).slice(0, 10).map(a => a.href).filter(h => h && h.startsWith('http'));
                }}
            """)

            # ìƒˆ ë§í¬ë§Œ í•„í„°ë§
            new_links = [l for l in links if l not in self.seen_urls][:3]
            print(f"   Found {len(new_links)} new links")

            return new_links

        except Exception as e:
            print(f"   âŒ Error fetching {source['name']}: {e}")
            return []

    async def learn_from_url(self, url: str, source_name: str) -> dict:
        """URLì—ì„œ í•™ìŠµ"""
        if url in self.seen_urls:
            return {"url": url, "skipped": True}

        self.seen_urls.add(url)
        start = time.time()
        print(f"ğŸ“– Learning: {url[:60]}...")

        try:
            await self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(1)

            title = await self.page.title()
            text = await self.page.evaluate("""
                () => {
                    const main = document.querySelector('article, main, .content, #content, .post-content') || document.body;
                    return main.innerText.replace(/\\s+/g, ' ').slice(0, 5000);
                }
            """)

            summary = await self.analyze_text(text) if len(text) > 100 else "ë‚´ìš© ë¶€ì¡±"

            result = {
                "url": url,
                "title": title,
                "source": source_name,
                "text_length": len(text),
                "summary": summary,
                "time": time.time() - start,
                "timestamp": datetime.now().isoformat()
            }

            self.learning_history.append(result)
            print(f"   âœ… {title[:45]} ({result['time']:.1f}s)")
            print(f"   ğŸ“ {summary[:80]}...")

            return result

        except Exception as e:
            print(f"   âŒ Error: {e}")
            return {"url": url, "error": str(e)}

    async def learn_from_feed(self, source_key: str):
        """íŠ¹ì • í”¼ë“œì—ì„œ í•™ìŠµ"""
        source = FEED_SOURCES.get(source_key, {})
        links = await self.fetch_feed_links(source_key)

        for link in links:
            await self.learn_from_url(link, source.get('name', source_key))
            await asyncio.sleep(0.5)

    async def continuous_learn(self, sources: list = None, interval_minutes: int = 2):
        """ì§€ì† í•™ìŠµ - ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ì†ŒìŠ¤ ìˆœí™˜"""
        if sources is None:
            sources = list(FEED_SOURCES.keys())

        print(f"\nğŸ”„ Trend Learning Started")
        print(f"   Sources: {', '.join(sources)}")
        print(f"   Interval: {interval_minutes} min")

        cycle = 1
        source_idx = 0

        while True:
            print(f"\n{'='*50}")
            print(f"ğŸ“š Cycle {cycle}")

            # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ì†ŒìŠ¤ 2ê°œì”© ì²˜ë¦¬
            for _ in range(2):
                source = sources[source_idx % len(sources)]
                await self.learn_from_feed(source)
                source_idx += 1

            # í†µê³„
            print(f"\nğŸ“Š Stats: {len(self.learning_history)} pages learned, {len(self.seen_urls)} unique URLs")

            # ì €ì¥
            self.save_history()

            print(f"â³ Waiting {interval_minutes} min...")
            await asyncio.sleep(interval_minutes * 60)
            cycle += 1

    def save_history(self, path: str = "/home/kim/agi/trend_history.json"):
        """í•™ìŠµ ê¸°ë¡ ì €ì¥"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.learning_history, f, ensure_ascii=False, indent=2)

    async def close(self):
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()


async def main():
    import argparse
    parser = argparse.ArgumentParser(description="Trend Learner - ì‹¤ì‹œê°„ í”¼ë“œ í•™ìŠµ")
    parser.add_argument("--sources", nargs="+",
                       default=["hackernews", "reddit_ml", "naver_news"],
                       help="Sources: " + ", ".join(FEED_SOURCES.keys()))
    parser.add_argument("--headless", action="store_true")
    parser.add_argument("--interval", type=int, default=2, help="Interval in minutes")
    args = parser.parse_args()

    learner = TrendLearner(headless=args.headless)

    try:
        await learner.setup_browser()
        await learner.setup_model()
        await learner.continuous_learn(args.sources, args.interval)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Stopping...")
    finally:
        learner.save_history()
        await learner.close()


if __name__ == "__main__":
    asyncio.run(main())
